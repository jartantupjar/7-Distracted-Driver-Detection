{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import random\n",
    "import cv2\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD,Adagrad,Adam, Nadam\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "import matplotlib.pyplot as plt     \n",
    "from keras.preprocessing import image    \n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D,GlobalMaxPooling2D\n",
    "from keras.layers import BatchNormalization,Dropout, Flatten, Dense, Input\n",
    "from keras.models import Sequential,Model,model_from_json\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import optimizers\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from keras.callbacks import EarlyStopping, Callback,ModelCheckpoint\n",
    "from sklearn.model_selection import KFold \n",
    "from keras.regularizers import l1,l2,l1_l2\n",
    "from keras.layers.noise import AlphaDropout\n",
    "\n",
    "class PlotLosses(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show();\n",
    "\n",
    "def create_submission(predictions, test_id, loss,model_name):\n",
    "    print('Started building csv file')\n",
    "    result = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result.insert(loc=0, column='img', value=test_id)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir('submissions'):\n",
    "        os.mkdir('submissions')\n",
    "    suffix = str(round(loss, 6)) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    subfile = os.path.join('subm', 'submission' +model_name+ '_'  +suffix + '.csv')\n",
    "    result.to_csv(subfile, index=False)\n",
    "    print(\"successfully created submission\")   \n",
    "    \n",
    "def cache_data(data, path):\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "\n",
    "def restore_data(path):\n",
    "    data = dict()\n",
    "    if os.path.isfile(path):\n",
    "        file = open(path, 'rb')\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "def read_resize_images(path):\n",
    "    img = cv2.imread(path)\n",
    "    img_rows, img_cols=224,224\n",
    "    resized = cv2.resize(img, (img_rows, img_cols))\n",
    "    return resized\n",
    "\n",
    "def get_driver_data():\n",
    "    dr = dict()\n",
    "    clss = dict()\n",
    "    path ='Data/driver_imgs_list.csv'\n",
    "    print('Read drivers data')\n",
    "    f = open(path, 'r')\n",
    "    line = f.readline()\n",
    "    while (1):\n",
    "        line = f.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        arr = line.strip().split(',')\n",
    "        dr[arr[2]] = arr[0]\n",
    "        if arr[0] not in clss.keys():\n",
    "            clss[arr[0]] = [(arr[1], arr[2])]\n",
    "        else:\n",
    "            clss[arr[0]].append((arr[1], arr[2]))\n",
    "    f.close()\n",
    "    return dr, clss\n",
    "\n",
    "def load_train():\n",
    "    driver_file=[]\n",
    "    driver_target=[]\n",
    "    driver_id=[]\n",
    "    driver_file_id=[]\n",
    "    driver_data, dr_class = get_driver_data()\n",
    "    print('driver data and class sample',len(driver_data),len(dr_class))\n",
    "    for j in range(10):\n",
    "        print('Load folder c{}'.format(j))\n",
    "        path = os.path.join( 'Data', 'imgs', 'train', 'c' + str(j), '*.jpg')\n",
    "        data = glob(path)\n",
    "        for p in data:\n",
    "            \n",
    "            driver_file.append(read_resize_images(p))\n",
    "            driver_target.append(j)\n",
    "            base = os.path.basename(p)\n",
    "            driver_file_id.append(base)\n",
    "            driver_id.append(driver_data[base])\n",
    "            \n",
    "    unique_drivers = sorted(list(set(driver_id)))\n",
    "    print('Unique drivers: {}'.format(len(unique_drivers)))\n",
    "    print(unique_drivers)\n",
    "    return driver_file,driver_target,driver_file_id,driver_id,unique_drivers\n",
    "\n",
    "def load_test():\n",
    "    path='Data/imgs/test/*'\n",
    "    data=glob(path)\n",
    "    driver_file=[]\n",
    "    driver_id=[]  \n",
    "    for p in data:\n",
    "        driver_file.append(read_resize_images(p))\n",
    "        driver_id.append(os.path.basename(p))\n",
    "        \n",
    "    return driver_file,driver_id\n",
    "\n",
    "def split_list(l,size):\n",
    "    return [l[i*len(l) // size: (i+1)*len(l) // size] for i in range(size)]\n",
    "\n",
    "def load_test_parts(part,splits):\n",
    "    path='Data/imgs/test/*'\n",
    "    data=glob(path)\n",
    "    driver_file=[]\n",
    "    driver_id=[]  \n",
    "    test_chunks=split_list(data,splits)\n",
    "    \n",
    "    for p in test_chunks[part]:\n",
    "        driver_file.append(read_resize_images(p))\n",
    "        driver_id.append(os.path.basename(p))\n",
    "\n",
    "    return driver_file,driver_id\n",
    "\n",
    "def append_chunk(main, part):\n",
    "    for p in part:\n",
    "        main.append(p)\n",
    "        \n",
    "    return main\n",
    "\n",
    "def get_selected_drivers(train_data, train_target, driver_id, driver_list):\n",
    "    data = []\n",
    "    target = []\n",
    "    index = []\n",
    "   \n",
    "    for i in range(len(train_data)):\n",
    "        if driver_id[i] in driver_list:\n",
    "            data.append(train_data[i])\n",
    "            target.append(train_target[i])\n",
    "    del train_data\n",
    "    del train_target\n",
    "    del driver_id\n",
    "    del driver_list\n",
    "    data = np.array(data,dtype=np.uint8)\n",
    "    target = np.array(target,dtype=np.uint8)\n",
    "    return data, target\n",
    "\n",
    "def get_val_selected_drivers(train_data, train_target, driver_id, driver_list):\n",
    "    data = []\n",
    "    target = []\n",
    "    index = []\n",
    "   \n",
    "    for i in range(len(train_data)):\n",
    "        if driver_id[i] in driver_list:\n",
    "            data.append(train_data[i])\n",
    "            target.append(train_target[i])\n",
    "            index.append(i)\n",
    "    del train_data\n",
    "    del train_target\n",
    "    del driver_id\n",
    "    del driver_list\n",
    "    data = np.array(data,dtype=np.float32)\n",
    "    target = np.array(target,dtype=np.uint8)\n",
    "    index = np.array(index)\n",
    "    return data, target, index\n",
    "\n",
    "    \n",
    "def merge_prediction_outputs(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    \n",
    "    return a.tolist()\n",
    "\n",
    "img_rows=224\n",
    "img_cols=224\n",
    "img_channel=3  \n",
    "\n",
    "batch_size=32\n",
    "nfolds=10\n",
    "epochs=20\n",
    "build_submission=True\n",
    "model_list=[\n",
    "  # 'ResNet50'\n",
    "   # ,\n",
    "     'VGG19'\n",
    "           ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def VGG19_arch():\n",
    "    base_model = VGG19(input_shape=(224,224,3),weights='imagenet', include_top=False)\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    \n",
    "    predictions = Dense(10, activation='softmax')(x)  \n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=SGD(lr=0.0009,momentum=0.9,nesterov=True),loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def ResNet50_arch():\n",
    "    base_model = ResNet50(input_shape=(224,224,3),weights='imagenet', include_top=False)\n",
    "\n",
    "    x = base_model.output\n",
    "  #  x = GlobalAveragePooling2D()(x)\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dense(100, activation='relu')(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    predictions = Dense(10, activation='softmax')(x)  \n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=SGD(lr=0.001, momentum=0.9,nesterov=True), loss='categorical_crossentropy')\n",
    "   \n",
    "    return model\n",
    " \n",
    "\n",
    "def get_model_arch(model_name):\n",
    "    model_arch=None\n",
    "    if model_name=='VGG19':\n",
    "        model_arch=VGG19_arch()\n",
    "    elif model_name=='ResNet50':\n",
    "        model_arch=ResNet50_arch()\n",
    "    else:\n",
    "        print('No model with such name')\n",
    "    return model_arch\n",
    "    \n",
    "def get_validation_predictions(train_data, predictions_valid):\n",
    "    pv = []\n",
    "    for i in range(len(train_data)):\n",
    "        pv.append(predictions_valid[i])\n",
    "    \n",
    "    del train_data\n",
    "    del predictions_valid\n",
    "    return pv\n",
    "\n",
    "\n",
    "\n",
    "def build_model(model_name,nfolds,epochs):\n",
    "    print('Model: ', model_name)\n",
    "    \n",
    "    random_state = 51\n",
    "   \n",
    "    cache_path= os.path.join('cache','train_224.dat')\n",
    "    if not os.path.isfile(cache_path):\n",
    "        print('building train cache')\n",
    "        train_files,train_targets,train_id, driver_id, unique_drivers = load_train()\n",
    "        cache_data((train_files,train_targets,train_id, driver_id, unique_drivers),cache_path)\n",
    "        print('train cache built')           \n",
    "    else:\n",
    "        print('Restore train from cache')\n",
    "        (train_files,train_targets,train_id, driver_id, unique_drivers)=restore_data(cache_path)\n",
    "    \n",
    "    \n",
    "\n",
    "    kf = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    num_fold = 0\n",
    "    sum_score = 0\n",
    "    for train_drivers, test_drivers in kf.split(unique_drivers):\n",
    "        model = get_model_arch(model_name)\n",
    "        unique_list_train = [unique_drivers[i] for i in train_drivers]\n",
    "        X_train, Y_train  = get_selected_drivers(train_files, train_targets, driver_id, unique_list_train)\n",
    "        unique_list_valid = [unique_drivers[i] for i in test_drivers]\n",
    "        X_valid, Y_valid, test_index = get_val_selected_drivers(train_files, train_targets, driver_id, unique_list_valid)\n",
    "\n",
    "        num_fold += 1\n",
    "        print('Start KFold number {} of {}'.format(num_fold, nfolds))\n",
    "        print('Split train: ', len(X_train), len(Y_train))\n",
    "        print('Split valid: ', len(X_valid), len(Y_valid))\n",
    "        print('Train drivers: ', unique_list_train)\n",
    "        print('Test drivers: ', unique_list_valid)\n",
    "        \n",
    "        X_valid/=255\n",
    "        \n",
    "        \n",
    "        train_datagen=ImageDataGenerator(\n",
    "          rescale=1./255,\n",
    "           zoom_range=0.4,\n",
    "           width_shift_range=0.4,\n",
    "           height_shift_range=0.4, \n",
    "           rotation_range=0.2\n",
    "            )\n",
    "\n",
    "        \n",
    "        Y_train=np_utils.to_categorical(Y_train,10)\n",
    "        Y_valid=np_utils.to_categorical(Y_valid,10)\n",
    "            \n",
    "        kfold_weights_path = os.path.join('cache', 'weights_kfold_augmented_'+ model_name + '_' +  str(num_fold) + '.h5')\n",
    "        if not os.path.isfile(kfold_weights_path) :\n",
    "            plot_losses = PlotLosses()\n",
    "            callbacks = [\n",
    "                \n",
    "                EarlyStopping(monitor='val_loss', patience=8, verbose=0),\n",
    "                plot_losses,\n",
    "                ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=2),\n",
    "                \n",
    "            ]\n",
    "            X_train=X_train.astype('float32')\n",
    "            print('X train shape',X_train.shape)\n",
    "            \n",
    "            print('--finished pre processing--')\n",
    "            \n",
    "          #  if not os.path.isdir(os.path.join('Data', 'augmented')):\n",
    "            #    os.mkdir(os.path.join('Data', 'augmented'))\n",
    "            history=model.fit_generator(train_datagen.flow(X_train,Y_train,batch_size=batch_size,seed=random_state),\n",
    "                            validation_data=(X_valid,Y_valid),\n",
    "                    epochs=epochs ,use_multiprocessing=False,steps_per_epoch=len(X_train)/batch_size,\n",
    "                            callbacks=callbacks,verbose=0) \n",
    "        \n",
    "        if os.path.isfile(kfold_weights_path):\n",
    "            model.load_weights(kfold_weights_path)\n",
    "\n",
    "        del X_train\n",
    "        \n",
    "        predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=0)\n",
    "        score = log_loss(Y_valid, predictions_valid)\n",
    "        print('Score log_loss: ', score)\n",
    "        sum_score += score*len(test_index)\n",
    "        \n",
    "            \n",
    "        del test_index\n",
    "        del train_datagen\n",
    "        del model\n",
    "        del predictions_valid\n",
    "        del X_valid\n",
    "        \n",
    "    score = sum_score/len(train_files)\n",
    "\n",
    "    print('(validation score) Final log_loss: {}, nfolds: {} epoch: {}'.format(score, nfolds, epochs))\n",
    "\n",
    "    \n",
    "    return predictions_valid,train_targets\n",
    "\n",
    "\n",
    "\n",
    "def train_selected_models(model_list,nfolds,epochs,build_submission):\n",
    "    multi_model=[]\n",
    "    train_targets=None\n",
    "    for i in range(len(model_list)):\n",
    "        result,train_targets=build_model(model_list[i],nfolds,epochs)\n",
    "        multi_model.append(result)\n",
    "        \n",
    "    num_models=len(multi_model)    \n",
    "    print('num_models: ',num_models)\n",
    "    result=merge_prediction_outputs(multi_model, num_models)\n",
    "    score=log_loss(train_targets,result)\n",
    "    print(\"Log_loss train set {} model/s: {} \".format(num_models, score)) \n",
    "    \n",
    "    if build_submission == True:\n",
    "        test_set_submission(model_list,nfolds,score)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_selected_models(model_list,nfolds,epochs,build_submission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  VGG19\n",
      "Fold No.  1\n",
      "iteration:  0\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  1\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  2\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  3\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  4\n",
      "Restore test from cache\n",
      "test length (15946, 224, 224, 3)\n",
      "prediction length  79726\n",
      "test id length  79726\n",
      "Fold No.  2\n",
      "iteration:  0\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  1\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  2\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  3\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  4\n",
      "Restore test from cache\n",
      "test length (15946, 224, 224, 3)\n",
      "prediction length  79726\n",
      "test id length  79726\n",
      "Fold No.  3\n",
      "iteration:  0\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  1\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  2\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  3\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  4\n",
      "Restore test from cache\n",
      "test length (15946, 224, 224, 3)\n",
      "prediction length  79726\n",
      "test id length  79726\n",
      "Fold No.  4\n",
      "iteration:  0\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  1\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  2\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  3\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  4\n",
      "Restore test from cache\n",
      "test length (15946, 224, 224, 3)\n",
      "prediction length  79726\n",
      "test id length  79726\n",
      "Fold No.  5\n",
      "iteration:  0\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  1\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  2\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  3\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  4\n",
      "Restore test from cache\n",
      "test length (15946, 224, 224, 3)\n",
      "prediction length  79726\n",
      "test id length  79726\n",
      "Fold No.  6\n",
      "iteration:  0\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  1\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  2\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  3\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  4\n",
      "Restore test from cache\n",
      "test length (15946, 224, 224, 3)\n",
      "prediction length  79726\n",
      "test id length  79726\n",
      "Fold No.  7\n",
      "iteration:  0\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  1\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  2\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  3\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  4\n",
      "Restore test from cache\n",
      "test length (15946, 224, 224, 3)\n",
      "prediction length  79726\n",
      "test id length  79726\n",
      "Fold No.  8\n",
      "iteration:  0\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  1\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  2\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  3\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  4\n",
      "Restore test from cache\n",
      "test length (15946, 224, 224, 3)\n",
      "prediction length  79726\n",
      "test id length  79726\n",
      "Fold No.  9\n",
      "iteration:  0\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  1\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  2\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  3\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  4\n",
      "Restore test from cache\n",
      "test length (15946, 224, 224, 3)\n",
      "prediction length  79726\n",
      "test id length  79726\n",
      "Fold No.  10\n",
      "iteration:  0\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  1\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  2\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  3\n",
      "Restore test from cache\n",
      "test length (15945, 224, 224, 3)\n",
      "iteration:  4\n",
      "Restore test from cache\n",
      "test length (15946, 224, 224, 3)\n",
      "prediction length  79726\n",
      "test id length  79726\n",
      "num_models:  1\n",
      "Model:  VGG19\n",
      "Restore test split from cache\n",
      "post pixel check log_loss:  0.139613957338\n",
      "post pixel check log_loss:  0.103564294118\n",
      "post pixel check log_loss:  0.0670347122127\n",
      "post pixel check log_loss:  0.132860065792\n",
      "post pixel check log_loss:  0.131965792502\n",
      "post pixel check log_loss:  0.05311031017\n",
      "post pixel check log_loss:  0.123067036986\n",
      "post pixel check log_loss:  0.146162133324\n",
      "post pixel check log_loss:  0.0132334102192\n",
      "post pixel check log_loss:  0.179770724037\n",
      "post pixel mean log_loss:  0.053596454996\n",
      "num_models:  1\n",
      "Test set model mean log loss 0.053596454996\n",
      "Started building csv file\n",
      "successfully created submission\n"
     ]
    }
   ],
   "source": [
    "def run_model_submission(model_name,nfolds):    \n",
    "    \n",
    "    print('Model: ', model_name)\n",
    "    \n",
    "    #nfolds=5\n",
    "    num_fold=0\n",
    "    test_splits=5\n",
    "    full_pred=[]\n",
    "    full_test_id=[]\n",
    "    for i in range(nfolds):\n",
    "\n",
    "        model= get_model_arch(model_name)\n",
    "        num_fold+=1\n",
    "        print('Fold No. ',num_fold)\n",
    "        kfold_weights_path = os.path.join('cache', 'weights_kfold_augmented_' + model_name + '_' + str(num_fold) + '.h5')\n",
    "        model.load_weights(kfold_weights_path)\n",
    "        \n",
    "        predictions=[]\n",
    "        \n",
    "        for x in range(test_splits):\n",
    "            print('iteration: ',x)\n",
    "           # cache_path=None\n",
    "            part=x+1\n",
    "            cache_path= os.path.join('cache','test_224_part'+str(part)+'.dat')\n",
    "\n",
    "            if not os.path.isfile(cache_path):\n",
    "                print('building test cache')\n",
    "                test_files,test_id = load_test_parts(x,test_splits)\n",
    "                cache_data((test_files,test_id),cache_path)\n",
    "                print('test cache built')\n",
    "            else:\n",
    "                print('Restore test from cache')\n",
    "                (test_files,test_id)=restore_data(cache_path)\n",
    "\n",
    "            test_files=np.array(test_files,dtype=np.uint8)    \n",
    "            test_files = test_files.reshape(test_files.shape[0], img_rows, img_cols,img_channel)\n",
    "            test_files = test_files.astype('float32')\n",
    "\n",
    "            print('test length',test_files.shape)\n",
    "            test_files/=255\n",
    "\n",
    "            prediction=model.predict(test_files,  batch_size=batch_size)\n",
    "            predictions=append_chunk(predictions,prediction)\n",
    "            if num_fold==1:\n",
    "                full_test_id=append_chunk(full_test_id,test_id)\n",
    "            \n",
    "            del test_files\n",
    "        \n",
    "        print('prediction length ',len(predictions))\n",
    "        print('test id length ',len(full_test_id))\n",
    "        full_pred.append(predictions)\n",
    "        \n",
    "        \n",
    "        \n",
    "    result=merge_prediction_outputs(full_pred, nfolds)   \n",
    "       \n",
    "    return result,full_test_id\n",
    "\n",
    "def test_set_submission(model_list,nfolds,score):\n",
    "    multi_model=[]\n",
    "    test_id=None\n",
    "    model_text=''\n",
    "    for i in range(len(model_list)):\n",
    "        result,test_id =run_model_submission(model_list[i],nfolds)\n",
    "        multi_model.append(result)\n",
    "        create_submission(result,test_id,score,model_list[i])\n",
    "        model_text=model_text+'_'+model_list[i]\n",
    "    num_models=len(multi_model)   \n",
    "    print('num_models: ',num_models)\n",
    "    result=merge_prediction_outputs(multi_model, num_models)\n",
    "    \n",
    "    create_submission(result,test_id,score,model_text) \n",
    "\n",
    "\n",
    "\n",
    "test_set_submission(model_list,nfolds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
