{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder c0\n",
      "Load folder c1\n",
      "Load folder c2\n",
      "Load folder c3\n",
      "Load folder c4\n",
      "Load folder c5\n",
      "Load folder c6\n",
      "Load folder c7\n",
      "Load folder c8\n",
      "Load folder c9\n",
      "22424\n",
      "Train shape: (22424, 96, 128, 1)\n",
      "22424 train samples\n",
      "Split train:  14351\n",
      "Split valid:  4485\n",
      "Split holdout:  3588\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 96, 128, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 64, 64)        4160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 24, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 32, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 24576)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24576)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               12288500  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 12,330,646\n",
      "Trainable params: 12,330,646\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 11480 samples, validate on 2871 samples\n",
      "Epoch 1/3\n",
      "11480/11480 [==============================] - 26s - loss: 1.2496 - val_loss: 0.2528\n",
      "Epoch 2/3\n",
      "11480/11480 [==============================] - 23s - loss: 0.2114 - val_loss: 0.0958\n",
      "Epoch 3/3\n",
      "11480/11480 [==============================] - 23s - loss: 0.0992 - val_loss: 0.0597\n",
      "Score:  0.0606264359012\n",
      "Score holdout:  0.0584903652473\n",
      "3456/3588 [===========================>..] - ETA: 0sScore log_loss:  0.0584903478028\n",
      "Read test images\n",
      "Read 7972 images from 79726\n",
      "Read 15944 images from 79726\n",
      "Read 23916 images from 79726\n",
      "Read 31888 images from 79726\n",
      "Read 39860 images from 79726\n",
      "Read 47832 images from 79726\n",
      "Read 55804 images from 79726\n",
      "Read 63776 images from 79726\n",
      "Read 71748 images from 79726\n",
      "Read 79720 images from 79726\n",
      "Test shape: (79726, 96, 128, 1)\n",
      "79726 test samples\n",
      "79726/79726 [==============================] - 20s    \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2016)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def get_im(path):\n",
    "    # Load as grayscale\n",
    "    img = cv2.imread(path, 0)\n",
    "    # Reduce size\n",
    "    resized = cv2.resize(img, (128, 96))\n",
    "    return resized\n",
    "\n",
    "\n",
    "def load_train():\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    print('Read train images')\n",
    "    counter=0\n",
    "    for j in range(10):\n",
    "        print('Load folder c{}'.format(j))\n",
    "        path = os.path.join( 'Data', 'imgs', 'train', 'c' + str(j), '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            \n",
    "            img = get_im(fl)\n",
    "            X_train.append(img)\n",
    "            y_train.append(j)\n",
    "            counter=counter+1\n",
    "    print(counter)\n",
    "    #print(y_train[1])\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def load_test():\n",
    "    print('Read test images')\n",
    "    path = os.path.join('Data', 'imgs', 'test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    thr = math.floor(len(files)/10)\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "        total += 1\n",
    "        if total%thr == 0:\n",
    "            print('Read {} images from {}'.format(total, len(files)))\n",
    "\n",
    "    return X_test, X_test_id\n",
    "\n",
    "\n",
    "def cache_data(data, path):\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "    else:\n",
    "        print('Directory doesnt exists')\n",
    "\n",
    "\n",
    "def restore_data(path):\n",
    "    data = dict()\n",
    "    if os.path.isfile(path):\n",
    "        file = open(path, 'rb')\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    json_string = model.to_json()\n",
    "    if not os.path.isdir('cache'):\n",
    "        os.mkdir('cache')\n",
    "    open(os.path.join('cache', 'architecture.json'), 'w').write(json_string)\n",
    "    model.save_weights(os.path.join('cache', 'model_weights.h5'), overwrite=True)\n",
    "\n",
    "\n",
    "def read_model():\n",
    "    model = model_from_json(open(os.path.join('cache', 'architecture.json')).read())\n",
    "    model.load_weights(os.path.join('cache', 'model_weights.h5'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def split_validation_set(train, target, test_size):\n",
    "    random_state = 51\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def split_validation_set_with_hold_out(train, target, test_size):\n",
    "    random_state = 51\n",
    "    train, X_test, target, y_test = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    X_train, X_holdout, y_train, y_holdout = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, X_holdout, y_train, y_test, y_holdout\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_id, loss):\n",
    "    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir('subm'):\n",
    "        os.mkdir('subm')\n",
    "    suffix = str(round(loss, 6)) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\n",
    "    result1.to_csv(sub_file, index=False)\n",
    "\n",
    "\n",
    "# The same as log_loss\n",
    "def mlogloss(target, pred):\n",
    "    score = 0.0\n",
    "    for i in range(len(pred)):\n",
    "        pp = pred[i]\n",
    "        for j in range(len(pp)):\n",
    "            prob = pp[j]\n",
    "            if prob < 1e-15:\n",
    "                prob = 1e-15\n",
    "            score += target[i][j] * math.log(prob)\n",
    "    return -score/len(pred)\n",
    "\n",
    "\n",
    "def validate_holdout(model, holdout, target):\n",
    "    predictions = model.predict(holdout, batch_size=128, verbose=1)\n",
    "    score = log_loss(target, predictions)\n",
    "    print('Score log_loss: ', score)\n",
    "    # score = model.evaluate(holdout, target, show_accuracy=True, verbose=0)\n",
    "    # print('Score holdout: ', score)\n",
    "    # score = mlogloss(target, predictions)\n",
    "    # print('Score : mlogloss', score)\n",
    "    return score\n",
    "\n",
    "\n",
    "cache_path = os.path.join('cache', 'sample','train.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    train_data, train_target = load_train()\n",
    "    cache_data((train_data, train_target), cache_path)\n",
    "else:\n",
    "    print('Restore train from cache!')\n",
    "    (train_data, train_target) = restore_data(cache_path)\n",
    "\n",
    "batch_size = 20\n",
    "nb_classes = 10\n",
    "nb_epoch = 3\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 96, 128\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "train_data = np.array(train_data, dtype=np.uint8)\n",
    "train_target = np.array(train_target, dtype=np.uint8)\n",
    "train_data = train_data.reshape(train_data.shape[0],  img_rows, img_cols,1)\n",
    "# train_data = train_data.transpose((0, 3, 1, 2))\n",
    "train_target = np_utils.to_categorical(train_target, nb_classes)\n",
    "train_data = train_data.astype('float32')\n",
    "train_data /= 255\n",
    "print('Train shape:', train_data.shape)\n",
    "print(train_data.shape[0], 'train samples')\n",
    "\n",
    "X_train, X_test, X_holdout, Y_train, Y_test, Y_holdout = split_validation_set_with_hold_out(train_data, train_target, 0.2)\n",
    "print('Split train: ', len(X_train))\n",
    "print('Split valid: ', len(X_test))\n",
    "print('Split holdout: ', len(X_holdout))\n",
    "\n",
    "\n",
    "model_from_cache = 0\n",
    "if model_from_cache == 1:\n",
    "    model = read_model()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "else:\n",
    "\n",
    "    model = Sequential()\n",
    "  \n",
    "    model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(img_rows,img_cols,1)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    #model.add(GlobalAveragePooling2D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "    '''\n",
    "    model.fit(train_data, train_target, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "              show_accuracy=True, verbose=1, validation_split=0.1)\n",
    "    '''\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "               verbose=1, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Score: ', score)\n",
    "score = model.evaluate(X_holdout, Y_holdout, verbose=0)\n",
    "print('Score holdout: ', score)\n",
    "validate_holdout(model, X_holdout, Y_holdout)\n",
    "save_model(model)\n",
    "\n",
    "cache_path = os.path.join('cache','sample', 'test.dat')\n",
    "if not os.path.isfile(cache_path):\n",
    "    test_data, test_id = load_test()\n",
    "    cache_data((test_data, test_id), cache_path)\n",
    "else:\n",
    "    print('Restore test from cache!')\n",
    "    (test_data, test_id) = restore_data(cache_path)\n",
    "\n",
    "test_data = np.array(test_data, dtype=np.uint8)\n",
    "test_data = test_data.reshape(test_data.shape[0],  img_rows, img_cols,1)\n",
    "# test_data = test_data.transpose((0, 3, 1, 2))\n",
    "test_data = test_data.astype('float32')\n",
    "test_data /= 255\n",
    "print('Test shape:', test_data.shape)\n",
    "print(test_data.shape[0], 'test samples')\n",
    "predictions = model.predict(test_data, batch_size=128, verbose=1)\n",
    "\n",
    "create_submission(predictions, test_id, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
